{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#*1) clone repo*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZJe8K0dxVOZS"
      },
      "id": "ZJe8K0dxVOZS"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/womeramichay/elta_home_assignment\n",
        "\n",
        "%cd elta_home_assignment\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peb09j85ULSS",
        "outputId": "322a98f5-464b-4967-8b7e-8f509d49c5cb"
      },
      "id": "peb09j85ULSS",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'elta_home_assignment'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 58 (delta 24), reused 6 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (58/58), 79.84 KiB | 11.41 MiB/s, done.\n",
            "Resolving deltas: 100% (24/24), done.\n",
            "/content/elta_home_assignment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **2)Kaggle auth**"
      ],
      "metadata": {
        "id": "VNwGzJxMWIgG"
      },
      "id": "VNwGzJxMWIgG"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngwm2mWnWMIP",
        "outputId": "e69e6965-4acc-41ee-f0d0-b40c06bc9d16"
      },
      "id": "ngwm2mWnWMIP",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **3) Download the data**"
      ],
      "metadata": {
        "id": "97c1wrsDWTzs"
      },
      "id": "97c1wrsDWTzs"
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data/raw\n",
        "!kaggle competitions download -c titanic -p data/raw\n",
        "!unzip -o data/raw/titanic.zip -d data/raw\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5aDK6ZzWXxW",
        "outputId": "57877934-fe0c-4513-f387-5cc58613585d"
      },
      "id": "K5aDK6ZzWXxW",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 4, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/__init__.py\", line 6, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 434, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n",
            "unzip:  cannot find or open data/raw/titanic.zip, data/raw/titanic.zip.zip or data/raw/titanic.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84171096",
      "metadata": {
        "id": "84171096"
      },
      "source": [
        "---\n",
        "\n",
        "#**4) quick look at the data.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "734d3074",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "734d3074",
        "outputId": "6cee3911-5080-4bc2-d2bc-6c983217ac4a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/raw/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1532843846.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mDATA_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/raw/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/raw/train.csv'"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 100)\n",
        "pd.set_option(\"display.max_rows\", 200)\n",
        "\n",
        "# data downloaded via Kaggle into data/raw/\n",
        "DATA_PATH = Path(\"data/raw/train.csv\")\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test size\n",
        "df[\"Survived\"].value_counts()\n"
      ],
      "metadata": {
        "id": "ticdWoQ1Z_YH"
      },
      "id": "ticdWoQ1Z_YH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test distribution\n",
        "df[\"Survived\"].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "FAWeZRyIaICW"
      },
      "id": "FAWeZRyIaICW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #missing values by col\n"
      ],
      "metadata": {
        "id": "b1KTfMDzatE8"
      },
      "id": "b1KTfMDzatE8"
    },
    {
      "cell_type": "code",
      "source": [
        "(df.isna().mean() * 100).sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "QJZN1NCRawPs"
      },
      "id": "QJZN1NCRawPs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes\n"
      ],
      "metadata": {
        "id": "v_xumV12bO5I"
      },
      "id": "v_xumV12bO5I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cross corelation for numeric cols**"
      ],
      "metadata": {
        "id": "JeEZti1QdRht"
      },
      "id": "JeEZti1QdRht"
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select numeric columns including target\n",
        "numeric_cols = [\n",
        "    \"Survived\",\n",
        "    \"Pclass\",\n",
        "    \"Age\",\n",
        "    \"SibSp\",\n",
        "    \"Parch\",\n",
        "    \"Fare\",\n",
        "]\n",
        "\n",
        "corr = df[numeric_cols].corr()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation matrix (numeric features)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YxiL6p2XdYxC"
      },
      "id": "YxiL6p2XdYxC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature selection decisions\n",
        "---\n",
        "1) After reviewing the EDA results and the correlation heatmap, I decided to drop the following columns:\n",
        "\n",
        "\n",
        "*   PassengerId - Identifier only an arbitrary value that have no corelation to survival rate.\n",
        "*   Name - a text that have no direct correlation to survival rate.\n",
        "*   Ticket - a text that have no clear stracture, not realy learnable.\n",
        "*   Cabin - missing to many values.\n",
        "\n",
        "2) Handling missing values::\n",
        "  * age : Missing values are imputed using the median (obust to skewed distributions).\n",
        "  * Embarked: Missing values are filled with the most frequent category before encoding, to avoid introducing an implicit “missing” category.\n",
        "\n",
        "\n",
        "3) Categorical encoding: Non-numeric categorical features (**Sex**, **Embarked**) are represented using one-hot encoding.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sHjQD7JSb8I5"
      },
      "id": "sHjQD7JSb8I5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select features to keep and creat sample + lables + One hot encode categorical features"
      ],
      "metadata": {
        "id": "GH6HqfNayp6i"
      },
      "id": "GH6HqfNayp6i"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# fix categorical missing values (explicit, clean)\n",
        "df = df.copy()\n",
        "df[\"Embarked\"] = df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0])\n",
        "\n",
        "# Select the features we keep (dropping others by omission)\n",
        "features = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\", \"Embarked\"]\n",
        "X = df[features]\n",
        "y = df[\"Survived\"]\n",
        "\n",
        "# one hot encode categorical columns\n",
        "X_encoded = pd.get_dummies(X,columns=[\"Sex\", \"Embarked\"],drop_first=True)\n",
        "\n",
        "# impute numeric missing values (Age, Fare if needed)\n",
        "numeric_cols = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
        "num_imputer = SimpleImputer(strategy=\"median\")\n",
        "X_encoded[numeric_cols] = num_imputer.fit_transform(X_encoded[numeric_cols])\n",
        "\n",
        "# train-validation split (stratified)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_encoded, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# sanity checks\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"Any NaNs left?\", X_train.isna().any().any(), X_val.isna().any().any())\n",
        "# print(\"Encoded columns:\", list(X_encoded.columns))\n",
        "\n",
        "X_encoded.head()\n"
      ],
      "metadata": {
        "id": "ryqRDVQvcAHD"
      },
      "id": "ryqRDVQvcAHD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#**Simple train using Logistic Regretion BASLINE**\n",
        "just to have a baseline and see F1 score.\n",
        "it will help understand which  new features to generate."
      ],
      "metadata": {
        "id": "onv6kOabzuYL"
      },
      "id": "onv6kOabzuYL"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def eval_logreg_f1(X, y):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_val)\n",
        "    return f1_score(y_val, y_pred)\n"
      ],
      "metadata": {
        "id": "7qMXdW9jzt5j"
      },
      "id": "7qMXdW9jzt5j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_f1 = eval_logreg_f1(X_encoded, y)\n",
        "print(\"baseline_f1:\", round(baseline_f1, 4))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FYTUP5Vu-2Yn"
      },
      "id": "FYTUP5Vu-2Yn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#looking at SVM BASLINE"
      ],
      "metadata": {
        "id": "RjnAIDf27pOp"
      },
      "id": "RjnAIDf27pOp"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def eval_svm_f1(X, y):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    model = LinearSVC(max_iter=5000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_val)\n",
        "    return f1_score(y_val, y_pred)\n"
      ],
      "metadata": {
        "id": "QS58LtfABQ7U"
      },
      "id": "QS58LtfABQ7U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg_f1 = eval_logreg_f1(X_encoded, y)\n",
        "svm_f1 = eval_svm_f1(X_encoded, y)\n",
        "\n",
        "print(\"Logistic Regression F1:\", round(logreg_f1, 4))\n",
        "print(\"Linear SVM F1:        \", round(svm_f1, 4))\n",
        "print(\"Delta (SVM - LR):     \", round(svm_f1 - logreg_f1, 4))\n"
      ],
      "metadata": {
        "id": "Fr5X_uywBTxZ"
      },
      "id": "Fr5X_uywBTxZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here i will test some hypotheses about new features selection"
      ],
      "metadata": {
        "id": "VETHkZuEBv0O"
      },
      "id": "VETHkZuEBv0O"
    },
    {
      "cell_type": "code",
      "source": [
        "# Target statistics: survival rate by group\n",
        "\n",
        "def survival_rate(col):\n",
        "    return df.groupby(col)[\"Survived\"].mean().sort_values(ascending=False)\n",
        "\n",
        "survival_rate(\"Sex\")\n",
        "survival_rate(\"Pclass\")\n",
        "survival_rate(\"Embarked\")\n"
      ],
      "metadata": {
        "id": "UsA3wY3PB2xG"
      },
      "id": "UsA3wY3PB2xG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Age bins → survival rate\n",
        "df[\"AgeBin\"] = pd.cut(\n",
        "    df[\"Age\"],\n",
        "    bins=[0, 12, 18, 35, 60, 100],\n",
        "    labels=[\"Child\", \"Teen\", \"YoungAdult\", \"Adult\", \"Senior\"]\n",
        ")\n",
        "\n",
        "survival_rate(\"AgeBin\")\n"
      ],
      "metadata": {
        "id": "dVc3fvewB8PI"
      },
      "id": "dVc3fvewB8PI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def survival_rate(col):\n",
        "    return df.groupby(col)[\"Survived\"].mean().sort_values(ascending=False)\n",
        "\n",
        "# Inspect categorical signals\n",
        "print(\"Sex:\\n\", survival_rate(\"Sex\"), \"\\n\")\n",
        "print(\"Pclass:\\n\", survival_rate(\"Pclass\"), \"\\n\")\n",
        "print(\"Embarked:\\n\", survival_rate(\"Embarked\"), \"\\n\")\n"
      ],
      "metadata": {
        "id": "rvfrV_qxCPd2"
      },
      "id": "rvfrV_qxCPd2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"IsChild\"] = (df[\"Age\"] < 18).astype(int)\n",
        "\n",
        "survival_rate(\"IsChild\")\n"
      ],
      "metadata": {
        "id": "oVTvX4AdCZk-"
      },
      "id": "oVTvX4AdCZk-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_child = X_encoded.copy()\n",
        "X_child[\"IsChild\"] = df[\"IsChild\"].fillna(0)\n",
        "\n",
        "child_f1 = eval_logreg_f1(X_child, y)\n",
        "\n",
        "print(f\"baseline_f1: {baseline_f1:.4f}\")\n",
        "print(f\"child_f1:    {child_f1:.4f}\")\n",
        "print(f\"delta:       {child_f1 - baseline_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "R_l0cvPOCh3H"
      },
      "id": "R_l0cvPOCh3H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Sex × Pclass interaction (new cell)\n",
        "X_inter = X_encoded.copy()\n",
        "\n",
        "# create 3 interaction columns: male AND each Pclass\n",
        "# (works because you already have: Pclass numeric + Sex_male dummy)\n",
        "for p in [1, 2, 3]:\n",
        "    X_inter[f\"SexMale_x_Pclass{p}\"] = (X_inter[\"Sex_male\"].astype(int) * (X_inter[\"Pclass\"] == p).astype(int))\n",
        "\n",
        "# 3) evaluate using the SAME function + baseline_f1 you already computed\n",
        "inter_f1 = eval_logreg_f1(X_inter, y)\n",
        "\n",
        "print(f\"baseline_f1: {baseline_f1:.4f}\")\n",
        "print(f\"inter_f1:    {inter_f1:.4f}\")\n",
        "print(f\"delta:       {inter_f1 - baseline_f1:.4f}\")\n",
        "\n",
        "# quick sanity (optional): see the new columns exist\n",
        "X_inter[[c for c in X_inter.columns if c.startswith(\"SexMale_x_\")]].head()\n"
      ],
      "metadata": {
        "id": "9xdDRseDDqTj"
      },
      "id": "9xdDRseDDqTj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NEW FEATURE TEST: IsAlone\n",
        "\n",
        "X_alone = X_encoded.copy()\n",
        "\n",
        "# Create IsAlone from SibSp and Parch\n",
        "X_alone[\"IsAlone\"] = ((X_alone[\"SibSp\"] + X_alone[\"Parch\"]) == 0).astype(int)\n",
        "\n",
        "# Evaluate\n",
        "alone_f1 = eval_logreg_f1(X_alone, y)\n",
        "\n",
        "print(f\"baseline_f1: {baseline_f1:.4f}\")\n",
        "print(f\"isalone_f1:  {alone_f1:.4f}\")\n",
        "print(f\"delta:       {alone_f1 - baseline_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "df69W_CbELNl"
      },
      "id": "df69W_CbELNl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random forest basrline"
      ],
      "metadata": {
        "id": "nFKPcU2EKLa7"
      },
      "id": "nFKPcU2EKLa7"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODEL CHANGE TEST: Random Forest (new cell) ---\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# same split logic as before\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_encoded,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_val)\n",
        "\n",
        "rf_f1 = f1_score(y_val, y_pred)\n",
        "\n",
        "print(f\"logreg_baseline_f1: {baseline_f1:.4f}\")\n",
        "print(f\"random_forest_f1:   {rf_f1:.4f}\")\n",
        "print(f\"delta:              {rf_f1 - baseline_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "DODH-wjsJwfd"
      },
      "id": "DODH-wjsJwfd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# same split for fair comparison\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=6,          # <-- key change (was too shallow before)\n",
        "    min_samples_leaf=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_val)\n",
        "\n",
        "rf_f1 = f1_score(y_val, y_pred)\n",
        "\n",
        "print(f\"logreg_baseline_f1: {baseline_f1:.4f}\")\n",
        "print(f\"random_forest_f1:  {rf_f1:.4f}\")\n",
        "print(f\"delta:             {rf_f1 - baseline_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "wVYPAqzWKmVy"
      },
      "id": "wVYPAqzWKmVy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model selection conclusion\n",
        "\n",
        "Several engineered features (IsChild, IsAlone, interaction terms) were evaluated\n",
        "using controlled F1 comparisons and did not improve performance.\n",
        "\n",
        "A Random Forest model was tested to capture potential non-linear interactions,\n",
        "but it underperformed the Logistic Regression baseline, indicating that the\n",
        "remaining predictive signal is largely linear and already well captured.\n",
        "\n",
        "Therefore, Logistic Regression will be our baseline model.\n"
      ],
      "metadata": {
        "id": "NK_rhMsVK20s"
      },
      "id": "NK_rhMsVK20s"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NN Model"
      ],
      "metadata": {
        "id": "XXwpFd1bLy4R"
      },
      "id": "XXwpFd1bLy4R"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "def _make_mlp(input_dim,\n",
        "              hidden_units=(128, 64),\n",
        "              activation=\"relu\",          # \"relu\" | \"gelu\" | \"leakyrelu\"\n",
        "              dropout=0.3,\n",
        "              batch_norm=True,\n",
        "              optimizer_name=\"adam\",      # \"adam\" | \"adamw\" | \"sgd\"\n",
        "              lr=1e-3,\n",
        "              weight_decay=1e-4):\n",
        "    inputs = tf.keras.Input(shape=(input_dim,))\n",
        "    x = inputs\n",
        "\n",
        "    for units in hidden_units:\n",
        "        x = tf.keras.layers.Dense(units)(x)\n",
        "        if batch_norm:\n",
        "            x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        if activation == \"leakyrelu\":\n",
        "            x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
        "        else:\n",
        "            x = tf.keras.layers.Activation(activation)(x)  # relu / gelu\n",
        "\n",
        "        if dropout and dropout > 0:\n",
        "            x = tf.keras.layers.Dropout(dropout)(x)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    opt = optimizer_name.lower()\n",
        "    if opt == \"adam\":\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    elif opt == \"adamw\":\n",
        "        optimizer = tf.keras.optimizers.AdamW(learning_rate=lr, weight_decay=weight_decay)\n",
        "    elif opt == \"sgd\":\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
        "    else:\n",
        "        raise ValueError(\"optimizer_name must be one of: adam, adamw, sgd\")\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def eval_nn_cv_f1(X, y,\n",
        "                  optimizer_name=\"adam\",\n",
        "                  lr=1e-3,\n",
        "                  hidden_units=(128, 64),\n",
        "                  activation=\"relu\",\n",
        "                  dropout=0.3,\n",
        "                  batch_norm=True,\n",
        "                  weight_decay=1e-4,\n",
        "                  epochs=80,\n",
        "                  batch_size=64,\n",
        "                  patience=10,\n",
        "                  n_splits=5,\n",
        "                  random_state=42,\n",
        "                  verbose=0):\n",
        "    X = np.asarray(X, dtype=np.float32)\n",
        "    y = np.asarray(y, dtype=np.int32)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    scores = []\n",
        "\n",
        "    for tr_idx, va_idx in skf.split(X, y):\n",
        "        X_tr, X_va = X[tr_idx], X[va_idx]\n",
        "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
        "\n",
        "        # scale (fit only on train fold)\n",
        "        scaler = StandardScaler()\n",
        "        X_tr = scaler.fit_transform(X_tr).astype(np.float32)\n",
        "        X_va = scaler.transform(X_va).astype(np.float32)\n",
        "\n",
        "        model = _make_mlp(\n",
        "            input_dim=X_tr.shape[1],\n",
        "            hidden_units=hidden_units,\n",
        "            activation=activation,\n",
        "            dropout=dropout,\n",
        "            batch_norm=batch_norm,\n",
        "            optimizer_name=optimizer_name,\n",
        "            lr=lr,\n",
        "            weight_decay=weight_decay,\n",
        "        )\n",
        "\n",
        "        cb = tf.keras.callbacks.EarlyStopping(\n",
        "            monitor=\"val_loss\", patience=patience, restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            validation_data=(X_va, y_va),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=[cb],\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "        proba = model.predict(X_va, verbose=0).ravel()\n",
        "        pred = (proba >= 0.5).astype(int)\n",
        "        scores.append(f1_score(y_va, pred))\n",
        "\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "    scores = np.array(scores, dtype=float)\n",
        "    return float(scores.mean()), float(scores.std())\n"
      ],
      "metadata": {
        "id": "KugsTZ6gLttS"
      },
      "id": "KugsTZ6gLttS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "# --- pick the right feature matrix automatically ---\n",
        "# (this avoids the X_encoded vs X_encoded typo problem)\n",
        "for name in [\"X_encoded\", \"X_encoded_fg\", \"X_inter\", \"X_alone\"]:\n",
        "    if name in globals():\n",
        "        X_nn = globals()[name]\n",
        "        X_nn_name = name\n",
        "        break\n",
        "else:\n",
        "    raise NameError(\"Couldn't find any of: X_encoded, X_encoded_fg, X_inter, X_alone. \"\n",
        "                    \"Make sure you created your encoded feature matrix first.\")\n",
        "\n",
        "print(\"Using feature matrix:\", X_nn_name, \"shape:\", X_nn.shape)\n",
        "\n",
        "# --- COARSE grid (big-picture spacing, not too huge) ---\n",
        "grid = {\n",
        "    \"optimizer_name\": [\"adam\", \"adamw\", \"sgd\"],\n",
        "    \"lr\": [3e-3, 1e-3, 3e-4],\n",
        "    \"hidden_units\": [(64,), (128, 64), (256, 128, 64)],\n",
        "    \"dropout\": [0.0, 0.3, 0.5],\n",
        "    \"activation\": [\"relu\", \"gelu\", \"leakyrelu\"],\n",
        "    \"batch_norm\": [True],\n",
        "}\n",
        "\n",
        "fixed = dict(\n",
        "    epochs=80,\n",
        "    batch_size=64,\n",
        "    patience=10,\n",
        "    n_splits=5,\n",
        "    random_state=42,\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "results = []\n",
        "keys = list(grid.keys())\n",
        "\n",
        "for values in itertools.product(*[grid[k] for k in keys]):\n",
        "    cfg = dict(zip(keys, values))\n",
        "    mean_f1, std_f1 = eval_nn_cv_f1(X_nn, y, **cfg, **fixed)\n",
        "    row = {**cfg, \"mean_f1\": mean_f1, \"std_f1\": std_f1}\n",
        "    results.append(row)\n",
        "    print(f\"done: {cfg} -> {mean_f1:.4f} ± {std_f1:.4f}\")\n",
        "\n",
        "df_res = pd.DataFrame(results).sort_values([\"mean_f1\", \"std_f1\"], ascending=[False, True]).reset_index(drop=True)\n",
        "df_res.head(15)\n"
      ],
      "metadata": {
        "id": "LC-bK9NpRG_b"
      },
      "id": "LC-bK9NpRG_b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}